{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_behavior_policy,\n",
    ")\n",
    "\n",
    "from obp.policy import IPWLearner, Random\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DoublyRobust as DR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人工データの生成\n",
    "\n",
    "p.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_rounds`: データ数\n",
    "- `n_actions`: 意思決定モデルが選択できる行動の数\n",
    "- `context`: 各データに対する特徴量\n",
    "- `action_context`: 行動を表現するone-hotベクトル\n",
    "- `action`: 過去の意思決定モデル`pi_b`によって選択された行動\n",
    "- `reward`: 過去の意思決定モデル`pi_b`によって選択された行動に対する報酬\n",
    "- `expected_reward`: 各行動に対する報酬の期待値\n",
    "- `pscore`: 過去の意思決定モデル`pi_b`による行動選択確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=3,\n",
    "    dim_context=3,\n",
    "    reward_function=logistic_reward_function,\n",
    "    behavior_policy_function=linear_behavior_policy,\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset.obtain_batch_bandit_feedback(n_rounds=10000)\n",
    "validation_data = dataset.obtain_batch_bandit_feedback(n_rounds=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"action_context\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(training_data[\"reward\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 意思決定モデルの学習\n",
    "\n",
    "p.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw_learner = IPWLearner(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw_learner.fit(\n",
    "    context=training_data[\"context\"],\n",
    "    action=training_data[\"action\"],\n",
    "    reward=training_data[\"reward\"],\n",
    "    pscore=training_data[\"pscore\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_choice_by_ipw_learner = ipw_learner.predict(\n",
    "    context=validation_data[\"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較のためにランダムに行動を選択する\n",
    "random = Random(n_actions=dataset.n_actions, random_state=12345)\n",
    "\n",
    "# バリデーションデータに対して行動選択確率を計算する -> これはランダムなので、行動選択確率は一様分布\n",
    "action_choice_by_random = random.compute_batch_action_dist(\n",
    "    n_rounds=validation_data[\"n_rounds\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 意思決定モデルの性能を評価する\n",
    "\n",
    "p.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR推定量を用いるために必要な目的変数予測モデルを構築する\n",
    "# opeモジュールに実装されているRegressionModelに好みの機械学習モデルを渡すことで、目的変数予測モデルを構築できる\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions,\n",
    "    base_model=LogisticRegression(C=100, random_state=12345),\n",
    ")\n",
    "\n",
    "# fit_predictメソッドを用いることで、構築した目的変数予測モデルを用いて、バリデーションデータに対する期待報酬を推定できる\n",
    "estimated_rewards_by_reg_model = regression_model.fit_predict(\n",
    "    context=validation_data[\"context\"],\n",
    "    action=validation_data[\"action\"],\n",
    "    reward=validation_data[\"reward\"],\n",
    "    n_folds=3,\n",
    "    random_state=12345,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
